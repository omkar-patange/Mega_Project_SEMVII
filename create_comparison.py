#!/usr/bin/env python3
"""
Create proper comparison results from individual test results
"""

import json
import os

def main():
    # Load both results
    with open('comparison_results_20251007_220719/userscale_results.json', 'r') as f:
        userscale_data = json.load(f)

    with open('comparison_results_20251007_220719/hpa_results.json', 'r') as f:
        hpa_data = json.load(f)

    # Calculate improvements
    us_metrics = userscale_data['metrics']
    hpa_metrics = hpa_data['metrics']

    throughput_improvement = ((us_metrics['throughput_rps'] - hpa_metrics['throughput_rps']) / max(hpa_metrics['throughput_rps'], 0.001)) * 100
    latency_improvement = ((hpa_metrics['avg_latency_ms'] - us_metrics['avg_latency_ms']) / max(hpa_metrics['avg_latency_ms'], 0.001)) * 100

    # Create comparison structure
    comparison_data = {
        'test_configuration': {
            'namespace': 'userscale',
            'concurrency': 10,
            'duration': 60,
            'matrix_size': 10000
        },
        'userscale_results': userscale_data,
        'hpa_results': hpa_data,
        'comparison_results': {
            'userscale': {
                'throughput_rps': us_metrics['throughput_rps'],
                'avg_latency_ms': us_metrics['avg_latency_ms'],
                'avg_replicas': 1.0  # We'll assume 1 replica for now
            },
            'hpa': {
                'throughput_rps': hpa_metrics['throughput_rps'],
                'avg_latency_ms': hpa_metrics['avg_latency_ms'],
                'avg_replicas': 1.0  # We'll assume 1 replica for now
            },
            'improvements': {
                'throughput_improvement_percent': throughput_improvement,
                'latency_improvement_percent': latency_improvement,
                'resource_efficiency_percent': 0.0  # No difference in replicas
            },
            'summary': {
                'userscale_better_throughput': throughput_improvement > 0,
                'userscale_better_latency': latency_improvement > 0,
                'userscale_more_efficient': False,
                'overall_winner': 'userscale' if throughput_improvement > 0 or latency_improvement > 0 else 'hpa'
            }
        }
    }

    # Save the combined results
    with open('comparison_results_20251007_220719/detailed_results.json', 'w') as f:
        json.dump(comparison_data, f, indent=2)

    print('âœ… Created detailed_results.json with proper comparison structure')
    print(f'ğŸ“Š Userscale Throughput: {us_metrics["throughput_rps"]:.2f} RPS')
    print(f'ğŸ“Š HPA Throughput: {hpa_metrics["throughput_rps"]:.2f} RPS')
    print(f'ğŸ“ˆ Throughput Improvement: {throughput_improvement:.2f}%')
    print(f'ğŸ† Overall Winner: {comparison_data["comparison_results"]["summary"]["overall_winner"].upper()}')

if __name__ == "__main__":
    main()
